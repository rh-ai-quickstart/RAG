# RAG Stack Configuration Values
# Copy this file to rag-values.yaml and customize the values
# The rag-values.yaml file is ignored by git to prevent accidental commits

# API Keys and Tokens
# Set your API keys and tokens here
# These will be used by the Helm chart during installation

# Hugging Face Token (required for model downloads)
# Get your token from: https://huggingface.co/settings/tokens
# Set this in the llm-service.secret.hf_token field below

# TAVILY Search API Key is configured in the llama-stack.secrets section below

# LLM Service Configuration
llm-service:
  secret:
    hf_token: ""
    enabled: true

# Global model configuration
global:
  models:
    # Pre-configured LLM models
    # To deploy the model, set the enabled flag to true 
    # For detailed model configuration visit the url of the `llm-service` helm chart at 
    # https://github.com/rh-ai-quickstart/ai-architecture-charts/blob/main/llm-service/helm/values.yaml 

    llama-3-2-1b-instruct:
      id: meta-llama/Llama-3.2-1B-Instruct
      enabled: false

    llama-3-1-8b-instruct:
      id: meta-llama/Llama-3.1-8B-Instruct
      enabled: false

    llama-3-2-1b-instruct-quantized:
      id: RedHatAI/Llama-3.2-1B-Instruct-quantized.w8a8
      enabled: false

    llama-3-2-3b-instruct:
      id: meta-llama/Llama-3.2-3B-Instruct
      enabled: false

    llama-3-3-70b-instruct:
      id: meta-llama/Llama-3.3-70B-Instruct
      enabled: false

    llama-3-3-70b-instruct-quantization-fp8:
      id: meta-llama/Llama-3.3-70B-Instruct
      enabled: false

    llama-guard-3-1b:
      id: meta-llama/Llama-Guard-3-1B
      enabled: false

    llama-guard-3-8b:
      id: meta-llama/Llama-Guard-3-8B
      enabled: false

    qwen-2-5-vl-3b-instruct:
      id: Qwen/Qwen2.5-VL-3B-Instruct
      enabled: false
    
    # To configure LlamaStack with remote llm, replace the id,
    # url and apiToken value and set enabled to true

    # remote-llm:
    #   id: custom-model-id
    #   url: https://custom-server-url/v1
    #   apiToken: fake-token
    #   enabled: false

    # To deploy custom models on RHOAI use the following code snippet

    # Example safety model configuration
    # custom-llm:
    #   id: meta-llama/Llama-3.2-1B-Instruct
    #   enabled: true
    #   device: "gpu"  # Options: "cpu", "gpu", "hpu"
    #   resources:
    #     limits:
    #       nvidia.com/gpu: "1"
    #   tolerations:
    #   - key: "nvidia.com/gpu"
    #     operator: Exists
    #     effect: NoSchedule
    #   args:
    #   - --max-model-len
    #   - "14336"
    
    # Example safety model configuration
    # custom-safety-guard:
    #   id: meta-llama/Llama-Guard-3-8B
    #   enabled: true
    #   registerShield: true
    #   device: "gpu"
    #   resources:
    #     limits:
    #       nvidia.com/gpu: "1"
    #   tolerations:
    #   - key: "nvidia.com/gpu"
    #     operator: Exists
    #     effect: NoSchedule

  # MCP servers configuration
  mcp-servers: {}

# Database Configuration (pgvector)
pgvector:
  secret:
    user: "postgres"
    password: "rag_password"
    dbname: "rag_blueprint"
    host: "pgvector"
    port: "5432"

# MinIO Configuration
minio:
  secret:
    user: "minio_rag_user"
    password: "minio_rag_password"
    host: "minio"
    port: "9000"
  
  # Upload sample files to the minio bucket 
  sampleFileUpload:
    enabled: true
    bucket: "documents"
    urls: 
    - "https://raw.githubusercontent.com/rh-ai-quickstart/RAG/refs/heads/main/notebooks/Zippity_Zoo_Grand_Invention.pdf"
    - "https://raw.githubusercontent.com/rh-ai-quickstart/RAG/refs/heads/main/notebooks/Zippity_Zoo_and_the_Town_of_Tumble_Town.pdf"
    - "https://raw.githubusercontent.com/rh-ai-quickstart/RAG/refs/heads/main/notebooks/Zippity_Zoo_and_the_Town_of_Whispering_Willows.pdf"

# Llama Stack Configuration
llama-stack:
  secrets:
    # TAVILY Search API Key for web search functionality
    TAVILY_SEARCH_API_KEY: "Paste-your-key-here"
    
    # Add other API keys as needed
    # OPENAI_API_KEY: "your_openai_key_here"
    # ANTHROPIC_API_KEY: "your_anthropic_key_here"

# Ingestion Pipeline Configuration
ingestion-pipeline:
  defaultPipeline:
    enabled: true
    # Source options: [S3, URL]
    source: "S3"
    # Embedding model for creating embeddings
    embedding_model: "all-MiniLM-L6-v2"
    # Name of the vector database
    name: "zippity-zoo-vector-db"
    # Version of the knowledge base
    version: "1.0"

    # S3 configuration for document storage
    S3:
      access_key_id: "minio_rag_user"
      secret_access_key: "minio_rag_password"
      bucket_name: "documents"
      endpoint_url: "http://minio:9000"
      region: "us-east-1"

# UI Configuration
replicaCount: 1

image:
  repository: "quay.io/rh-ai-quickstart/llamastack-dist-ui"
  pullPolicy: "Always"

env:
  - name: "LLAMA_STACK_ENDPOINT"
    value: "http://llamastack:8321"

