version: '3.8'

services:

  # Ollama server for local LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: rag-ollama
    restart: on-failure:50
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=*
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s  # Give more time for model download
    networks:
      - rag-network
    # Pull the model on startup
    entrypoint: >
      sh -c "
        ollama serve &
        sleep 10 &&
        ollama pull llama3.2:3b-instruct-fp16 &&
        wait
      "

  # Llama Stack server
  llamastack:
    image: llamastack/distribution-ollama:0.2.9
    platform: linux/amd64
    container_name: rag-llamastack
    restart: on-failure:50
    depends_on:
      ollama:
        condition: service_started
    environment:
      INFERENCE_MODEL: "llama3.2:3b-instruct-fp16"
      OLLAMA_URL: "http://ollama:11434"
      TAVILY_SEARCH_API_KEY: "${TAVILY_SEARCH_API_KEY:-}"  # Set this if you have a Tavily API key
    ports:
      - "8321:8321"
    volumes:
      - llamastack_data:/root/.llama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8321/"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s  # Give more time for startup
    networks:
      - rag-network


  # RAG Ingestion Service
  rag-ingestion:
    platform: linux/amd64
    build:
      context: ../../ingestion-service
      dockerfile: Containerfile
    container_name: rag-ingestion
    depends_on:
      llamastack:
        condition: service_started
    environment:
      INGESTION_CONFIG: "/config/ingestion-config.yaml"
    volumes:
      - ./ingestion-config.yaml:/config/ingestion-config.yaml:ro
    networks:
      - rag-network
    restart: "no"  # Run once and exit

  # RAG UI Frontend
  rag-ui:
    platform: linux/amd64
    build:
      context: ../../frontend
      dockerfile: Containerfile
      args:
        IMAGE_TAG: "0.2.9"
    container_name: rag-ui
    depends_on:
      llamastack:
        condition: service_started  # Changed from service_healthy
    environment:
      LLAMA_STACK_ENDPOINT: "http://llamastack:8321"
      TAVILY_SEARCH_API_KEY: "${TAVILY_SEARCH_API_KEY:-}"  # Set this if you have a Tavily API key
      RAG_QUESTION_SUGGESTIONS: |
        {
          "hr-vector-db-v1-0": [
            "What benefits does FantaCo provide?",
            "How many vacation days do employees get?",
            "What is the parental leave policy?",
            "What are the retirement benefits?",
            "How do I enroll in benefits?",
            "What is the employee assistance program?"
          ],
          "legal-vector-db-v1-0": [
            "What are the key contract terms?",
            "What is the liability clause?",
            "What are the termination conditions?",
            "What are the intellectual property rights?",
            "What is the dispute resolution process?",
            "What are the compliance requirements?"
          ],
          "sales-vector-db-v1-0": [
            "What is the sales process?",
            "How do I qualify leads?",
            "What are the pricing strategies?",
            "What is the commission structure?",
            "How do I handle customer objections?",
            "What are the territory assignments?"
          ],
          "procurement-vector-db-v1-0": [
            "What is the procurement process?",
            "How do I submit a purchase request?",
            "What are the approval requirements?",
            "Who are the approved vendors?",
            "What is the purchasing policy?",
            "How do I track my order?"
          ],
          "techsupport-vector-db-v1-0": [
            "How do I install CloudSync on Mac?",
            "How do I install CloudSync on Windows?",
            "How do I sync files between devices?",
            "How do I troubleshoot CloudSync sync issues?",
            "How do I install Linux on TechGear Pro Laptop?",
            "Where can I find video drivers for TechGear Pro?"
          ]
        }
    ports:
      - "8501:8501"
    networks:
      - rag-network
    restart: unless-stopped


volumes:
  ollama_data:
    driver: local
  llamastack_data:
    driver: local

networks:
  rag-network:
    driver: bridge